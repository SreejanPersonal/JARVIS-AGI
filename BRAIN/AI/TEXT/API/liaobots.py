import requests
import uuid
import os
from dotenv import load_dotenv; load_dotenv()

def generate(query: str, system_prompt: str="Keep your responses short and to the point", model_id: str="claude-instant-1"):
    """
    This function facilitates interaction with a variety of conversational AI models hosted on an external API endpoint. It allows users to submit queries and receive responses from different AI models based on their requirements.

    Parameters:
    - query (str): The text query or input message to be sent to the AI model.
    - system_prompt (str, optional): The prompt displayed to the user, guiding them on how to structure their responses. Defaults to "Keep your responses short and to the point".
    - model_id (str, optional): The identifier for the AI model to be used. Defaults to "claude-instant-1", which refers to a specific instance of the Claude model optimized for instant responses. Other available models are:
        - "gpt-3.5-turbo": A variant of the GPT-3.5 model with parameters suitable for fast responses.
        - "gpt-4-turbo-preview": A preview version of the GPT-4 model optimized for speed.
        - "gpt-4-plus": An enhanced version of the GPT-4 model with extended capabilities.
        - "claude-3-haiku-20240307": A specialized Claude model trained to generate Haiku poems.
        - "claude-2.1": A previous version of the Claude model with slightly different parameters.
        - "claude-instant-1" (default): The default Claude model optimized for instant responses.

    Returns:
    - str: The response from the AI model. This could be a text message generated by the model based on the input query.
    """

    # Define the URL
    url = "https://liaobots.work/api/chat"

    # Define headers
    headers = {
        "Cookie": "gkp2=9YygONFs0ygkiW2J21O7",
        "Origin": "https://liaobots.work",
        "Referer": "https://liaobots.work/en",
        "Sentry-Trace": f"{str(uuid.uuid4())}-{str(uuid.uuid4())}",
        "X-Auth-Code": os.environ.get("LIAOBOTS_X_AUTH_CODE"),
    }

    # Define the payload based on model_id
    model_details = {
        "gpt-3.5-turbo": {"maxLength": 48000, "tokenLimit": 14000, "model": "ChatGPT", "provider": "OpenAI", "context": "16K"},
        "gpt-4-turbo-preview": {"maxLength": 260000, "tokenLimit": 126000, "model": "ChatGPT", "provider": "OpenAI", "context": "128K"},
        "gpt-4-plus": {"maxLength": 130000, "tokenLimit": 31000, "model": "ChatGPT", "provider": "OpenAI", "context": "32K"},
        "claude-3-haiku-20240307": {"maxLength": 800000, "tokenLimit": 200000, "model": "Claude", "provider": "Anthropic", "context": "200K"},
        "claude-2.1": {"maxLength": 800000, "tokenLimit": 200000, "model": "Claude", "provider": "Anthropic", "context": "200K"},
        "claude-instant-1": {"maxLength": 400000, "tokenLimit": 100000, "model": "Claude", "provider": "Anthropic", "context": "100K"}
    }

    if model_id not in model_details:
        print(f"Model ID {model_id} is not recognized.")
        return

    payload = {
        "maxRounds": 1,
        "conversationId": str(uuid.uuid4()),
        "model": {
            "id": model_id,
            **model_details[model_id]
        },
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query}
        ],
        "key": "",
        "prompt": system_prompt
    }

    # Make the POST request
    response = requests.post(url, headers=headers, json=payload)

    # Check the response
    if response.status_code == 200:
        return response.text  # If you want to print the response JSON
    else:
        return f"API call failed with status code {response.status_code}\n Error : {response.text}"

if __name__ == "__main__":
    # Example usage:
    response = generate("what is yout name and who made you")
    print(response)
