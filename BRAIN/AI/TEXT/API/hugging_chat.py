import requests
import json
import os
from dotenv import load_dotenv

load_dotenv()

class HuggingChat_RE:
    def __init__(self, hf_chat: str = os.environ.get("HUGGING_CHAT_ID"), model: str = "meta-llama/Meta-Llama-3-70B-Instruct") -> None:
        """
        Initializes an instance of the HuggingChat_RE class.

        Parameters:
        - hf_chat (str): The Hugging Face chat token.
        - model (str): The name or path of the model to be used for the chat. Defaults to "meta-llama/Meta-Llama-3-70B-Instruct".

        Returns:
        - None: This is a constructor method and does not return anything.
        """

        self.hf_chat = hf_chat
        self.model = model
        self.headers = {
            "Cookie": f"hf-chat={self.hf_chat}",
        }
        self.conversationId = self.find_conversation_id()
        self.messageId = self.find_message_id()

    def find_conversation_id(self) -> str:
        """
        Finds and returns the conversation ID for the Hugging Face chat.

        Returns:
        - str: The conversation ID retrieved from the server response.
        """

        url = "https://huggingface.co/chat/conversation"
        payload = {"model": self.model}
        response = requests.post(url, json=payload, headers=self.headers).json()
        print("\033[92m" + "Initialised Conversation ID:", response['conversationId'] + "\033[0m")
        return response['conversationId']

    def find_message_id(self) -> str:
        """
        Finds and returns the message ID for the Hugging Face chat.

        Returns:
        - str: The message ID retrieved from the server response.
        """

        url = f"https://huggingface.co/chat/conversation/{self.conversationId}/__data.json?x-sveltekit-invalidated=11"
        response = requests.get(url, headers=self.headers).json()
        print("\033[92m" + "Initialised Message ID:", response['nodes'][1]['data'][3] + "\033[0m")
        return response['nodes'][1]['data'][3]

    def generate(self, query: str, web_search: bool = False, files=[], stream: bool = True) -> str:
        """
        Generates a response for the given query using the Hugging Face chat.

        Parameters:
        - query (str): The text query to generate a response for.
        - web_search (bool): A flag indicating whether to perform web search in the response generation process. Defaults to False.
        - files (List[str]): A list of file paths to include in the query. Defaults to an empty list.
        - stream (bool): A flag indicating whether to stream the response. Defaults to True.

        Returns:
        - str: The complete response generated by the chat.
        """

        url = f"https://huggingface.co/chat/conversation/{self.conversationId}"
        payload = {
            "inputs": query,
            "id": self.messageId,
            "is_retry": False,
            "is_continue": False,
            "web_search": web_search,
            "files": files
        }


        response = requests.post(url, json=payload, headers=self.headers, stream=True)
        complete_response = ""
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                try:
                    json_data = json.loads(chunk.decode("utf-8"))
                    if json_data['type'] == "stream":
                        if stream: print(json_data['token'], end="", flush=True)
                        complete_response += json_data['token']
                except:
                    continue

        return complete_response

# Example Usage
if __name__ == "__main__":

    hf_api = HuggingChat_RE(model="microsoft/Phi-3-mini-4k-instruct")
    # while True:
    #     query = '\033[96m' + input("\n> ") + '\033[0m'  # cyan
    #     response = hf_api.generate(query, web_search=False)
    #     # response = hf_api.generate(query, stream=False)
    #     # print(response)

    models = {
    "meta-llama/Meta-Llama-3-70B-Instruct": "https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct",
    "HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1": "https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1",
    "CohereForAI/c4ai-command-r-plus": "https://huggingface.co/CohereForAI/c4ai-command-r-plus",
    "mistralai/Mixtral-8x7B-Instruct-v0.1": "https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1",
    "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
    "google/gemma-1.1-7b-it": "https://huggingface.co/google/gemma-1.1-7b-it",
    "mistralai/Mistral-7B-Instruct-v0.2": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",
    "microsoft/Phi-3-mini-4k-instruct": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"
    }

    for name, url in models.items():
        print("\033[96m\n\n" + name + "\033[0m")

        hf_api = HuggingChat_RE(model=name)
        hf_api.generate("Who are you ?")